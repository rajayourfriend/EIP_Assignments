{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP4_Assignment5_PersonAttributes_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajayourfriend/EIP_Assignments/blob/master/EIP4_Assignment5_PersonAttributes_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY5uXJve__Ho",
        "colab_type": "text"
      },
      "source": [
        "### Changes\n",
        "1. pose to bodypose \n",
        "2. train_df to val_df\n",
        "3. VGG to resnet  \n",
        " (in_layer to neck) #anyways commented now\n",
        "4. Flatten to GAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPDdeCiRsrvP",
        "colab_type": "code",
        "outputId": "356ea9e5-819f-4d15-fe8b-6ea2b163c8f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "!nvidia-smi\n",
        "import keras\n",
        "print(\"\\n ... \\n\")\n",
        "print(\"Keras \", keras.__version__)\n",
        "print(\"\\n ... \\n\")\n",
        "!pip show tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 31 14:54:34 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ... \n",
            "\n",
            "Keras  2.2.5\n",
            "\n",
            " ... \n",
            "\n",
            "Name: tensorflow\n",
            "Version: 1.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: wheel, termcolor, numpy, tensorflow-estimator, wrapt, absl-py, protobuf, tensorboard, gast, keras-preprocessing, astor, grpcio, keras-applications, six, google-pasta, opt-einsum\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "76e0527c-697d-45fb-9a8a-edd55d943fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/datasets/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS-5c1CFss9r",
        "colab_type": "code",
        "outputId": "4108a3d2-0118-463c-ee04-c3f17977c028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.applications import resnet\n",
        "from keras.applications import resnet_v2\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
        "#keras.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
        "#keras.applications.resnet_v2.ResNet50V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "decaca89-f3ee-4c08-ed61-a5274e272f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "a8b97c0b-d410-4ec1-906d-88231d060722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3_MRj7CKKmT",
        "colab_type": "code",
        "outputId": "24a756a1-79ca-43b8-bd01-eceb7869a986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(\"one_hot_df = \\n\", one_hot_df)\n",
        "print(\"one_hot_df.type = \", type(one_hot_df))\n",
        "print(\"one_hot_df.columns = \", one_hot_df.columns)\n",
        "print(\"one_hot_df.columns.type = \", type(one_hot_df.columns))\n",
        "#RAJA one_hot_df.columns would contain all the newly created column_titles "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one_hot_df = \n",
            "               image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
            "0          resized/1.jpg              0  ...                        1              0\n",
            "1          resized/2.jpg              1  ...                        1              0\n",
            "2          resized/3.jpg              0  ...                        1              0\n",
            "3          resized/4.jpg              0  ...                        1              0\n",
            "4          resized/5.jpg              1  ...                        1              0\n",
            "...                  ...            ...  ...                      ...            ...\n",
            "13568  resized/13570.jpg              0  ...                        1              0\n",
            "13569  resized/13571.jpg              1  ...                        1              0\n",
            "13570  resized/13572.jpg              1  ...                        0              1\n",
            "13571  resized/13573.jpg              1  ...                        1              0\n",
            "13572  resized/13574.jpg              0  ...                        1              0\n",
            "\n",
            "[13573 rows x 28 columns]\n",
            "one_hot_df.type =  <class 'pandas.core.frame.DataFrame'>\n",
            "one_hot_df.columns =  Index(['image_path', 'gender_female', 'gender_male', 'imagequality_Average',\n",
            "       'imagequality_Bad', 'imagequality_Good', 'age_15-25', 'age_25-35',\n",
            "       'age_35-45', 'age_45-55', 'age_55+', 'weight_normal-healthy',\n",
            "       'weight_over-weight', 'weight_slightly-overweight',\n",
            "       'weight_underweight', 'carryingbag_Daily/Office/Work Bag',\n",
            "       'carryingbag_Grocery/Home/Plastic Bag', 'carryingbag_None',\n",
            "       'footwear_CantSee', 'footwear_Fancy', 'footwear_Normal',\n",
            "       'emotion_Angry/Serious', 'emotion_Happy', 'emotion_Neutral',\n",
            "       'emotion_Sad', 'bodypose_Back', 'bodypose_Front-Frontish',\n",
            "       'bodypose_Side'],\n",
            "      dtype='object')\n",
            "one_hot_df.columns.type =  <class 'pandas.core.indexes.base.Index'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuPoJPuMJJGJ",
        "colab_type": "code",
        "outputId": "e068d4df-6ffb-48bf-afae-24223f548f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "print(\"_imagequality_cols_.type = \", type(_imagequality_cols_))\n",
        "print(\"_imagequality_cols_ = \", _imagequality_cols_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_imagequality_cols_.type =  <class 'list'>\n",
            "_imagequality_cols_ =  ['imagequality_Average', 'imagequality_Bad', 'imagequality_Good']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USWyVFM_rQek",
        "colab_type": "code",
        "outputId": "93d7cb74-1043-4806-c85c-a7360323f94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "\n",
        "print(\"one_hot_df.columns.type = \", type(one_hot_df.columns))\n",
        "print(\"one_hot_df.columns = \", one_hot_df.columns)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one_hot_df.columns.type =  <class 'pandas.core.indexes.base.Index'>\n",
            "one_hot_df.columns =  Index(['image_path', 'gender_female', 'gender_male', 'imagequality_Average',\n",
            "       'imagequality_Bad', 'imagequality_Good', 'age_15-25', 'age_25-35',\n",
            "       'age_35-45', 'age_45-55', 'age_55+', 'weight_normal-healthy',\n",
            "       'weight_over-weight', 'weight_slightly-overweight',\n",
            "       'weight_underweight', 'carryingbag_Daily/Office/Work Bag',\n",
            "       'carryingbag_Grocery/Home/Plastic Bag', 'carryingbag_None',\n",
            "       'footwear_CantSee', 'footwear_Fancy', 'footwear_Normal',\n",
            "       'emotion_Angry/Serious', 'emotion_Happy', 'emotion_Neutral',\n",
            "       'emotion_Sad', 'bodypose_Back', 'bodypose_Front-Frontish',\n",
            "       'bodypose_Side'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"bodypose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOEDYDWJuhN",
        "colab_type": "code",
        "outputId": "bc04d537-1239-4740-fc08-c8da51fc7c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "one_hot_df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13573, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "12b0cd32-7366-4ee3-ec66-971f59e7581c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape\n",
        "#RAJA find why 28 (27 is the number of items in one hot encoded fields)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "346bd803-f9e3-4da1-9510-80290677e4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6439</th>\n",
              "      <td>resized/6440.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>resized/230.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11013</th>\n",
              "      <td>resized/11015.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6058</th>\n",
              "      <td>resized/6059.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2689</th>\n",
              "      <td>resized/2690.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "6439    resized/6440.jpg              0  ...                        1              0\n",
              "229      resized/230.jpg              1  ...                        1              0\n",
              "11013  resized/11015.jpg              0  ...                        1              0\n",
              "6058    resized/6059.jpg              1  ...                        1              0\n",
              "2689    resized/2690.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "93f2339f-aaab-4e25-ea91-f6c3db333b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'bodypose': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "fc9f3d62-a700-4aa8-c0bb-49ba6dd31405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "'''\n",
        "backbone = VGG16(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "'''\n",
        "#RAJA backbone -: neck -: tower -: head\n",
        "\n",
        "#rate = 1 - keep_prob\n",
        "backbone = resnet.ResNet50(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "#neck = Flatten(name=\"flatten\")(neck) #RAJA may check changing it to GAP\n",
        "neck = GlobalAveragePooling2D(name=\"gap\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    intermittent = Dense(128, activation=\"relu\")(in_layer)\n",
        "    intermittent = Dense(128, activation=\"relu\")(intermittent)\n",
        "    return intermittent\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "bodypose = build_head(\"bodypose\", build_tower(neck))\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, bodypose, emotion]\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "ae66b25c-7ea4-4851-c9ac-6e9474c55499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "a266f29f-aa14-4fa6-ad2f-be6fb823e282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=30,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.4571 - gender_output_loss: 0.5649 - image_quality_output_loss: 0.9249 - age_output_loss: 1.4327 - weight_output_loss: 0.9931 - bag_output_loss: 0.9025 - footwear_output_loss: 0.9070 - bodypose_output_loss: 0.8106 - emotion_output_loss: 0.9214 - gender_output_acc: 0.6983 - image_quality_output_acc: 0.5595 - age_output_acc: 0.3949 - weight_output_acc: 0.6264 - bag_output_acc: 0.5748 - footwear_output_acc: 0.5766 - bodypose_output_acc: 0.6482 - emotion_output_acc: 0.7041Epoch 1/30\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 7.4563 - gender_output_loss: 0.5644 - image_quality_output_loss: 0.9248 - age_output_loss: 1.4325 - weight_output_loss: 0.9926 - bag_output_loss: 0.9030 - footwear_output_loss: 0.9072 - bodypose_output_loss: 0.8107 - emotion_output_loss: 0.9210 - gender_output_acc: 0.6987 - image_quality_output_acc: 0.5593 - age_output_acc: 0.3949 - weight_output_acc: 0.6266 - bag_output_acc: 0.5744 - footwear_output_acc: 0.5766 - bodypose_output_acc: 0.6483 - emotion_output_acc: 0.7043 - val_loss: 7.8295 - val_gender_output_loss: 0.6115 - val_image_quality_output_loss: 1.0048 - val_age_output_loss: 1.4340 - val_weight_output_loss: 1.0324 - val_bag_output_loss: 0.9277 - val_footwear_output_loss: 1.1269 - val_bodypose_output_loss: 0.7617 - val_emotion_output_loss: 0.9305 - val_gender_output_acc: 0.7072 - val_image_quality_output_acc: 0.5489 - val_age_output_acc: 0.3861 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5786 - val_footwear_output_acc: 0.5096 - val_bodypose_output_acc: 0.6749 - val_emotion_output_acc: 0.7152\n",
            "Epoch 2/30\n",
            "360/360 [==============================] - 36s 101ms/step - loss: 6.9315 - gender_output_loss: 0.4783 - image_quality_output_loss: 0.8712 - age_output_loss: 1.3854 - weight_output_loss: 0.9565 - bag_output_loss: 0.8512 - footwear_output_loss: 0.8286 - bodypose_output_loss: 0.6758 - emotion_output_loss: 0.8846 - gender_output_acc: 0.7698 - image_quality_output_acc: 0.5819 - age_output_acc: 0.4057 - weight_output_acc: 0.6372 - bag_output_acc: 0.6143 - footwear_output_acc: 0.6249 - bodypose_output_acc: 0.7105 - emotion_output_acc: 0.7111 - val_loss: 7.8832 - val_gender_output_loss: 0.7150 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4773 - val_weight_output_loss: 1.1113 - val_bag_output_loss: 0.8943 - val_footwear_output_loss: 1.0516 - val_bodypose_output_loss: 0.7441 - val_emotion_output_loss: 0.9073 - val_gender_output_acc: 0.6799 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.5307 - val_bodypose_output_acc: 0.6880 - val_emotion_output_acc: 0.7152\n",
            "Epoch 3/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.7220 - gender_output_loss: 0.4522 - image_quality_output_loss: 0.8546 - age_output_loss: 1.3586 - weight_output_loss: 0.9382 - bag_output_loss: 0.8294 - footwear_output_loss: 0.7979 - bodypose_output_loss: 0.6192 - emotion_output_loss: 0.8720 - gender_output_acc: 0.7852 - image_quality_output_acc: 0.5959 - age_output_acc: 0.4182 - weight_output_acc: 0.6427 - bag_output_acc: 0.6305 - footwear_output_acc: 0.6410 - bodypose_output_acc: 0.7410 - emotion_output_acc: 0.7106Epoch 3/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 6.7223 - gender_output_loss: 0.4523 - image_quality_output_loss: 0.8546 - age_output_loss: 1.3587 - weight_output_loss: 0.9382 - bag_output_loss: 0.8298 - footwear_output_loss: 0.7982 - bodypose_output_loss: 0.6189 - emotion_output_loss: 0.8716 - gender_output_acc: 0.7848 - image_quality_output_acc: 0.5958 - age_output_acc: 0.4180 - weight_output_acc: 0.6424 - bag_output_acc: 0.6300 - footwear_output_acc: 0.6408 - bodypose_output_acc: 0.7413 - emotion_output_acc: 0.7107 - val_loss: 7.8038 - val_gender_output_loss: 0.6057 - val_image_quality_output_loss: 0.9981 - val_age_output_loss: 1.4491 - val_weight_output_loss: 1.0374 - val_bag_output_loss: 1.0226 - val_footwear_output_loss: 1.0380 - val_bodypose_output_loss: 0.7655 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.7248 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3831 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5827 - val_footwear_output_acc: 0.5343 - val_bodypose_output_acc: 0.6935 - val_emotion_output_acc: 0.7152\n",
            "Epoch 4/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 6.5647 - gender_output_loss: 0.4334 - image_quality_output_loss: 0.8420 - age_output_loss: 1.3400 - weight_output_loss: 0.9229 - bag_output_loss: 0.8112 - footwear_output_loss: 0.7787 - bodypose_output_loss: 0.5769 - emotion_output_loss: 0.8595 - gender_output_acc: 0.7977 - image_quality_output_acc: 0.6027 - age_output_acc: 0.4251 - weight_output_acc: 0.6435 - bag_output_acc: 0.6420 - footwear_output_acc: 0.6507 - bodypose_output_acc: 0.7574 - emotion_output_acc: 0.7109 - val_loss: 7.6875 - val_gender_output_loss: 0.7262 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4615 - val_weight_output_loss: 1.0495 - val_bag_output_loss: 0.9103 - val_footwear_output_loss: 0.9211 - val_bodypose_output_loss: 0.7493 - val_emotion_output_loss: 0.8916 - val_gender_output_acc: 0.6885 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.5786 - val_bodypose_output_acc: 0.7067 - val_emotion_output_acc: 0.7152\n",
            "Epoch 5/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.4246 - gender_output_loss: 0.4071 - image_quality_output_loss: 0.8280 - age_output_loss: 1.3217 - weight_output_loss: 0.9097 - bag_output_loss: 0.7920 - footwear_output_loss: 0.7633 - bodypose_output_loss: 0.5546 - emotion_output_loss: 0.8483 - gender_output_acc: 0.8141 - image_quality_output_acc: 0.6092 - age_output_acc: 0.4308 - weight_output_acc: 0.6465 - bag_output_acc: 0.6498 - footwear_output_acc: 0.6612 - bodypose_output_acc: 0.7712 - emotion_output_acc: 0.7115Epoch 5/30\n",
            "360/360 [==============================] - 36s 101ms/step - loss: 6.4256 - gender_output_loss: 0.4069 - image_quality_output_loss: 0.8278 - age_output_loss: 1.3213 - weight_output_loss: 0.9098 - bag_output_loss: 0.7924 - footwear_output_loss: 0.7632 - bodypose_output_loss: 0.5553 - emotion_output_loss: 0.8489 - gender_output_acc: 0.8141 - image_quality_output_acc: 0.6091 - age_output_acc: 0.4307 - weight_output_acc: 0.6464 - bag_output_acc: 0.6495 - footwear_output_acc: 0.6611 - bodypose_output_acc: 0.7713 - emotion_output_acc: 0.7112 - val_loss: 8.0312 - val_gender_output_loss: 0.6501 - val_image_quality_output_loss: 0.9898 - val_age_output_loss: 1.4733 - val_weight_output_loss: 1.1022 - val_bag_output_loss: 1.1456 - val_footwear_output_loss: 1.0917 - val_bodypose_output_loss: 0.6577 - val_emotion_output_loss: 0.9206 - val_gender_output_acc: 0.7293 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3841 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.5252 - val_bodypose_output_acc: 0.7288 - val_emotion_output_acc: 0.7152\n",
            "Epoch 6/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 6.2906 - gender_output_loss: 0.3914 - image_quality_output_loss: 0.8158 - age_output_loss: 1.3019 - weight_output_loss: 0.8947 - bag_output_loss: 0.7766 - footwear_output_loss: 0.7438 - bodypose_output_loss: 0.5285 - emotion_output_loss: 0.8379 - gender_output_acc: 0.8220 - image_quality_output_acc: 0.6184 - age_output_acc: 0.4428 - weight_output_acc: 0.6493 - bag_output_acc: 0.6576 - footwear_output_acc: 0.6713 - bodypose_output_acc: 0.7858 - emotion_output_acc: 0.7114 - val_loss: 8.1276 - val_gender_output_loss: 0.7621 - val_image_quality_output_loss: 0.9969 - val_age_output_loss: 1.4987 - val_weight_output_loss: 1.0857 - val_bag_output_loss: 0.9645 - val_footwear_output_loss: 1.0165 - val_bodypose_output_loss: 0.9029 - val_emotion_output_loss: 0.9003 - val_gender_output_acc: 0.6935 - val_image_quality_output_acc: 0.5388 - val_age_output_acc: 0.3846 - val_weight_output_acc: 0.6326 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.5565 - val_bodypose_output_acc: 0.6915 - val_emotion_output_acc: 0.7152\n",
            "Epoch 7/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 6.1746 - gender_output_loss: 0.3781 - image_quality_output_loss: 0.8052 - age_output_loss: 1.2840 - weight_output_loss: 0.8807 - bag_output_loss: 0.7605 - footwear_output_loss: 0.7334 - bodypose_output_loss: 0.5040 - emotion_output_loss: 0.8289 - gender_output_acc: 0.8338 - image_quality_output_acc: 0.6254 - age_output_acc: 0.4509 - weight_output_acc: 0.6541 - bag_output_acc: 0.6682 - footwear_output_acc: 0.6745 - bodypose_output_acc: 0.7945 - emotion_output_acc: 0.7118 - val_loss: 8.2775 - val_gender_output_loss: 0.5883 - val_image_quality_output_loss: 1.0088 - val_age_output_loss: 1.4863 - val_weight_output_loss: 1.1370 - val_bag_output_loss: 1.0836 - val_footwear_output_loss: 1.1868 - val_bodypose_output_loss: 0.8247 - val_emotion_output_loss: 0.9621 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.5489 - val_age_output_acc: 0.3569 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5832 - val_footwear_output_acc: 0.5166 - val_bodypose_output_acc: 0.7077 - val_emotion_output_acc: 0.7152\n",
            "Epoch 8/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 6.0559 - gender_output_loss: 0.3661 - image_quality_output_loss: 0.7927 - age_output_loss: 1.2702 - weight_output_loss: 0.8658 - bag_output_loss: 0.7475 - footwear_output_loss: 0.7087 - bodypose_output_loss: 0.4824 - emotion_output_loss: 0.8225 - gender_output_acc: 0.8363 - image_quality_output_acc: 0.6343 - age_output_acc: 0.4638 - weight_output_acc: 0.6568 - bag_output_acc: 0.6778 - footwear_output_acc: 0.6901 - bodypose_output_acc: 0.8071 - emotion_output_acc: 0.7126 - val_loss: 7.8396 - val_gender_output_loss: 0.5647 - val_image_quality_output_loss: 1.0657 - val_age_output_loss: 1.4766 - val_weight_output_loss: 1.0896 - val_bag_output_loss: 1.0504 - val_footwear_output_loss: 0.9788 - val_bodypose_output_loss: 0.6786 - val_emotion_output_loss: 0.9351 - val_gender_output_acc: 0.7596 - val_image_quality_output_acc: 0.5489 - val_age_output_acc: 0.3548 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.5932 - val_bodypose_output_acc: 0.7263 - val_emotion_output_acc: 0.7152\n",
            "Epoch 9/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 5.9049 - gender_output_loss: 0.3470 - image_quality_output_loss: 0.7814 - age_output_loss: 1.2463 - weight_output_loss: 0.8467 - bag_output_loss: 0.7256 - footwear_output_loss: 0.6906 - bodypose_output_loss: 0.4590 - emotion_output_loss: 0.8082 - gender_output_acc: 0.8427 - image_quality_output_acc: 0.6395 - age_output_acc: 0.4692 - weight_output_acc: 0.6632 - bag_output_acc: 0.6859 - footwear_output_acc: 0.6960 - bodypose_output_acc: 0.8136 - emotion_output_acc: 0.7127 - val_loss: 8.5015 - val_gender_output_loss: 0.7535 - val_image_quality_output_loss: 1.0333 - val_age_output_loss: 1.5164 - val_weight_output_loss: 1.1514 - val_bag_output_loss: 1.0197 - val_footwear_output_loss: 1.1942 - val_bodypose_output_loss: 0.9084 - val_emotion_output_loss: 0.9247 - val_gender_output_acc: 0.7188 - val_image_quality_output_acc: 0.5363 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6326 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.5444 - val_bodypose_output_acc: 0.6900 - val_emotion_output_acc: 0.7162\n",
            "Epoch 10/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 5.7664 - gender_output_loss: 0.3258 - image_quality_output_loss: 0.7624 - age_output_loss: 1.2213 - weight_output_loss: 0.8321 - bag_output_loss: 0.7085 - footwear_output_loss: 0.6795 - bodypose_output_loss: 0.4427 - emotion_output_loss: 0.7940 - gender_output_acc: 0.8605 - image_quality_output_acc: 0.6490 - age_output_acc: 0.4842 - weight_output_acc: 0.6683 - bag_output_acc: 0.6959 - footwear_output_acc: 0.7045 - bodypose_output_acc: 0.8211 - emotion_output_acc: 0.7138 - val_loss: 8.8105 - val_gender_output_loss: 1.1178 - val_image_quality_output_loss: 1.1017 - val_age_output_loss: 1.5324 - val_weight_output_loss: 1.1328 - val_bag_output_loss: 1.1381 - val_footwear_output_loss: 1.0035 - val_bodypose_output_loss: 0.8559 - val_emotion_output_loss: 0.9282 - val_gender_output_acc: 0.6618 - val_image_quality_output_acc: 0.5368 - val_age_output_acc: 0.3216 - val_weight_output_acc: 0.6316 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.5917 - val_bodypose_output_acc: 0.6971 - val_emotion_output_acc: 0.7152\n",
            "Epoch 10/30\n",
            "Epoch 11/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6243 - gender_output_loss: 0.3132 - image_quality_output_loss: 0.7499 - age_output_loss: 1.1917 - weight_output_loss: 0.8078 - bag_output_loss: 0.6853 - footwear_output_loss: 0.6615 - bodypose_output_loss: 0.4287 - emotion_output_loss: 0.7863 - gender_output_acc: 0.8653 - image_quality_output_acc: 0.6629 - age_output_acc: 0.5013 - weight_output_acc: 0.6765 - bag_output_acc: 0.7084 - footwear_output_acc: 0.7134 - bodypose_output_acc: 0.8301 - emotion_output_acc: 0.7174Epoch 11/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 5.6245 - gender_output_loss: 0.3128 - image_quality_output_loss: 0.7501 - age_output_loss: 1.1918 - weight_output_loss: 0.8079 - bag_output_loss: 0.6851 - footwear_output_loss: 0.6619 - bodypose_output_loss: 0.4286 - emotion_output_loss: 0.7864 - gender_output_acc: 0.8656 - image_quality_output_acc: 0.6624 - age_output_acc: 0.5015 - weight_output_acc: 0.6764 - bag_output_acc: 0.7087 - footwear_output_acc: 0.7134 - bodypose_output_acc: 0.8301 - emotion_output_acc: 0.7175 - val_loss: 9.3119 - val_gender_output_loss: 1.2128 - val_image_quality_output_loss: 1.1295 - val_age_output_loss: 1.6007 - val_weight_output_loss: 1.2341 - val_bag_output_loss: 0.9564 - val_footwear_output_loss: 1.3928 - val_bodypose_output_loss: 0.8231 - val_emotion_output_loss: 0.9626 - val_gender_output_acc: 0.6507 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.6195 - val_footwear_output_acc: 0.5030 - val_bodypose_output_acc: 0.7021 - val_emotion_output_acc: 0.7152\n",
            "Epoch 12/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 5.4588 - gender_output_loss: 0.3028 - image_quality_output_loss: 0.7334 - age_output_loss: 1.1690 - weight_output_loss: 0.7883 - bag_output_loss: 0.6618 - footwear_output_loss: 0.6454 - bodypose_output_loss: 0.3920 - emotion_output_loss: 0.7661 - gender_output_acc: 0.8701 - image_quality_output_acc: 0.6664 - age_output_acc: 0.5155 - weight_output_acc: 0.6857 - bag_output_acc: 0.7182 - footwear_output_acc: 0.7226 - bodypose_output_acc: 0.8454 - emotion_output_acc: 0.7236 - val_loss: 8.7229 - val_gender_output_loss: 0.6515 - val_image_quality_output_loss: 1.1708 - val_age_output_loss: 1.5931 - val_weight_output_loss: 1.2918 - val_bag_output_loss: 0.9659 - val_footwear_output_loss: 1.2771 - val_bodypose_output_loss: 0.8048 - val_emotion_output_loss: 0.9680 - val_gender_output_acc: 0.7515 - val_image_quality_output_acc: 0.5131 - val_age_output_acc: 0.3468 - val_weight_output_acc: 0.6295 - val_bag_output_acc: 0.6230 - val_footwear_output_acc: 0.5565 - val_bodypose_output_acc: 0.6925 - val_emotion_output_acc: 0.7152\n",
            "Epoch 13/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 5.2873 - gender_output_loss: 0.2779 - image_quality_output_loss: 0.7179 - age_output_loss: 1.1374 - weight_output_loss: 0.7725 - bag_output_loss: 0.6382 - footwear_output_loss: 0.6203 - bodypose_output_loss: 0.3738 - emotion_output_loss: 0.7495 - gender_output_acc: 0.8807 - image_quality_output_acc: 0.6753 - age_output_acc: 0.5312 - weight_output_acc: 0.6920 - bag_output_acc: 0.7371 - footwear_output_acc: 0.7363 - bodypose_output_acc: 0.8541 - emotion_output_acc: 0.7232 - val_loss: 9.4191 - val_gender_output_loss: 1.2015 - val_image_quality_output_loss: 1.1613 - val_age_output_loss: 1.5602 - val_weight_output_loss: 1.2638 - val_bag_output_loss: 1.0331 - val_footwear_output_loss: 1.0920 - val_bodypose_output_loss: 1.0497 - val_emotion_output_loss: 1.0575 - val_gender_output_acc: 0.6694 - val_image_quality_output_acc: 0.5358 - val_age_output_acc: 0.3513 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6134 - val_footwear_output_acc: 0.5635 - val_bodypose_output_acc: 0.6825 - val_emotion_output_acc: 0.7152\n",
            "Epoch 14/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 5.1195 - gender_output_loss: 0.2744 - image_quality_output_loss: 0.6941 - age_output_loss: 1.0995 - weight_output_loss: 0.7410 - bag_output_loss: 0.6210 - footwear_output_loss: 0.5972 - bodypose_output_loss: 0.3563 - emotion_output_loss: 0.7359 - gender_output_acc: 0.8816 - image_quality_output_acc: 0.6928 - age_output_acc: 0.5515 - weight_output_acc: 0.7049 - bag_output_acc: 0.7413 - footwear_output_acc: 0.7505 - bodypose_output_acc: 0.8622 - emotion_output_acc: 0.7322 - val_loss: 9.0110 - val_gender_output_loss: 0.5700 - val_image_quality_output_loss: 1.3757 - val_age_output_loss: 1.6745 - val_weight_output_loss: 1.2852 - val_bag_output_loss: 1.0575 - val_footwear_output_loss: 1.0392 - val_bodypose_output_loss: 0.9828 - val_emotion_output_loss: 1.0261 - val_gender_output_acc: 0.7661 - val_image_quality_output_acc: 0.4713 - val_age_output_acc: 0.2913 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6018 - val_bodypose_output_acc: 0.6885 - val_emotion_output_acc: 0.7147\n",
            "Epoch 15/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 4.9051 - gender_output_loss: 0.2391 - image_quality_output_loss: 0.6764 - age_output_loss: 1.0571 - weight_output_loss: 0.7155 - bag_output_loss: 0.5911 - footwear_output_loss: 0.5718 - bodypose_output_loss: 0.3390 - emotion_output_loss: 0.7151 - gender_output_acc: 0.9011 - image_quality_output_acc: 0.7032 - age_output_acc: 0.5768 - weight_output_acc: 0.7163 - bag_output_acc: 0.7596 - footwear_output_acc: 0.7548 - bodypose_output_acc: 0.8671 - emotion_output_acc: 0.7365 - val_loss: 9.7568 - val_gender_output_loss: 0.8627 - val_image_quality_output_loss: 1.2627 - val_age_output_loss: 1.8040 - val_weight_output_loss: 1.3111 - val_bag_output_loss: 1.3328 - val_footwear_output_loss: 1.1181 - val_bodypose_output_loss: 1.0566 - val_emotion_output_loss: 1.0089 - val_gender_output_acc: 0.7278 - val_image_quality_output_acc: 0.4481 - val_age_output_acc: 0.3594 - val_weight_output_acc: 0.6074 - val_bag_output_acc: 0.5943 - val_footwear_output_acc: 0.5917 - val_bodypose_output_acc: 0.6724 - val_emotion_output_acc: 0.7107\n",
            "Epoch 16/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 4.7276 - gender_output_loss: 0.2352 - image_quality_output_loss: 0.6530 - age_output_loss: 1.0271 - weight_output_loss: 0.6960 - bag_output_loss: 0.5562 - footwear_output_loss: 0.5429 - bodypose_output_loss: 0.3215 - emotion_output_loss: 0.6959 - gender_output_acc: 0.9015 - image_quality_output_acc: 0.7117 - age_output_acc: 0.5874 - weight_output_acc: 0.7226 - bag_output_acc: 0.7727 - footwear_output_acc: 0.7741 - bodypose_output_acc: 0.8772 - emotion_output_acc: 0.7411 - val_loss: 10.1724 - val_gender_output_loss: 1.0514 - val_image_quality_output_loss: 1.1782 - val_age_output_loss: 1.7262 - val_weight_output_loss: 1.2186 - val_bag_output_loss: 1.1901 - val_footwear_output_loss: 1.4263 - val_bodypose_output_loss: 1.2955 - val_emotion_output_loss: 1.0861 - val_gender_output_acc: 0.7067 - val_image_quality_output_acc: 0.5086 - val_age_output_acc: 0.3145 - val_weight_output_acc: 0.6124 - val_bag_output_acc: 0.6154 - val_footwear_output_acc: 0.5464 - val_bodypose_output_acc: 0.6643 - val_emotion_output_acc: 0.7132\n",
            "Epoch 17/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.5375 - gender_output_loss: 0.2152 - image_quality_output_loss: 0.6327 - age_output_loss: 0.9876 - weight_output_loss: 0.6657 - bag_output_loss: 0.5385 - footwear_output_loss: 0.5236 - bodypose_output_loss: 0.2986 - emotion_output_loss: 0.6756 - gender_output_acc: 0.9090 - image_quality_output_acc: 0.7231 - age_output_acc: 0.6099 - weight_output_acc: 0.7398 - bag_output_acc: 0.7849 - footwear_output_acc: 0.7853 - bodypose_output_acc: 0.8854 - emotion_output_acc: 0.7467Epoch 17/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 4.5382 - gender_output_loss: 0.2156 - image_quality_output_loss: 0.6323 - age_output_loss: 0.9877 - weight_output_loss: 0.6664 - bag_output_loss: 0.5383 - footwear_output_loss: 0.5234 - bodypose_output_loss: 0.2988 - emotion_output_loss: 0.6758 - gender_output_acc: 0.9088 - image_quality_output_acc: 0.7234 - age_output_acc: 0.6097 - weight_output_acc: 0.7395 - bag_output_acc: 0.7852 - footwear_output_acc: 0.7855 - bodypose_output_acc: 0.8852 - emotion_output_acc: 0.7467 - val_loss: 10.8009 - val_gender_output_loss: 1.3100 - val_image_quality_output_loss: 1.3743 - val_age_output_loss: 1.7574 - val_weight_output_loss: 1.4751 - val_bag_output_loss: 1.2218 - val_footwear_output_loss: 1.0669 - val_bodypose_output_loss: 1.4701 - val_emotion_output_loss: 1.1253 - val_gender_output_acc: 0.6809 - val_image_quality_output_acc: 0.4819 - val_age_output_acc: 0.2878 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6129 - val_footwear_output_acc: 0.5993 - val_bodypose_output_acc: 0.6593 - val_emotion_output_acc: 0.7142\n",
            "Epoch 18/30\n",
            "360/360 [==============================] - 35s 98ms/step - loss: 4.3035 - gender_output_loss: 0.2065 - image_quality_output_loss: 0.5989 - age_output_loss: 0.9433 - weight_output_loss: 0.6259 - bag_output_loss: 0.5062 - footwear_output_loss: 0.4977 - bodypose_output_loss: 0.2737 - emotion_output_loss: 0.6513 - gender_output_acc: 0.9152 - image_quality_output_acc: 0.7426 - age_output_acc: 0.6292 - weight_output_acc: 0.7555 - bag_output_acc: 0.7960 - footwear_output_acc: 0.7930 - bodypose_output_acc: 0.8944 - emotion_output_acc: 0.7576 - val_loss: 11.8261 - val_gender_output_loss: 1.4630 - val_image_quality_output_loss: 1.3208 - val_age_output_loss: 1.8382 - val_weight_output_loss: 1.4646 - val_bag_output_loss: 1.4999 - val_footwear_output_loss: 1.7171 - val_bodypose_output_loss: 1.4393 - val_emotion_output_loss: 1.0832 - val_gender_output_acc: 0.6638 - val_image_quality_output_acc: 0.4567 - val_age_output_acc: 0.3150 - val_weight_output_acc: 0.6008 - val_bag_output_acc: 0.6048 - val_footwear_output_acc: 0.5212 - val_bodypose_output_acc: 0.6628 - val_emotion_output_acc: 0.7011\n",
            "Epoch 19/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 4.1184 - gender_output_loss: 0.1853 - image_quality_output_loss: 0.5783 - age_output_loss: 0.8933 - weight_output_loss: 0.5991 - bag_output_loss: 0.4835 - footwear_output_loss: 0.4842 - bodypose_output_loss: 0.2647 - emotion_output_loss: 0.6300 - gender_output_acc: 0.9248 - image_quality_output_acc: 0.7523 - age_output_acc: 0.6530 - weight_output_acc: 0.7640 - bag_output_acc: 0.8059 - footwear_output_acc: 0.8007 - bodypose_output_acc: 0.8969 - emotion_output_acc: 0.7662 - val_loss: 11.7012 - val_gender_output_loss: 1.1433 - val_image_quality_output_loss: 1.4757 - val_age_output_loss: 1.8797 - val_weight_output_loss: 1.6213 - val_bag_output_loss: 1.6937 - val_footwear_output_loss: 1.2481 - val_bodypose_output_loss: 1.5503 - val_emotion_output_loss: 1.0890 - val_gender_output_acc: 0.7067 - val_image_quality_output_acc: 0.4073 - val_age_output_acc: 0.3453 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5922 - val_footwear_output_acc: 0.5680 - val_bodypose_output_acc: 0.6588 - val_emotion_output_acc: 0.6961\n",
            "Epoch 20/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.9007 - gender_output_loss: 0.1815 - image_quality_output_loss: 0.5454 - age_output_loss: 0.8535 - weight_output_loss: 0.5691 - bag_output_loss: 0.4571 - footwear_output_loss: 0.4530 - bodypose_output_loss: 0.2436 - emotion_output_loss: 0.5974 - gender_output_acc: 0.9268 - image_quality_output_acc: 0.7692 - age_output_acc: 0.6736 - weight_output_acc: 0.7751 - bag_output_acc: 0.8182 - footwear_output_acc: 0.8177 - bodypose_output_acc: 0.9092 - emotion_output_acc: 0.7748Epoch 20/30\n",
            "360/360 [==============================] - 35s 98ms/step - loss: 3.9001 - gender_output_loss: 0.1815 - image_quality_output_loss: 0.5452 - age_output_loss: 0.8540 - weight_output_loss: 0.5685 - bag_output_loss: 0.4565 - footwear_output_loss: 0.4530 - bodypose_output_loss: 0.2436 - emotion_output_loss: 0.5977 - gender_output_acc: 0.9267 - image_quality_output_acc: 0.7692 - age_output_acc: 0.6733 - weight_output_acc: 0.7754 - bag_output_acc: 0.8184 - footwear_output_acc: 0.8178 - bodypose_output_acc: 0.9093 - emotion_output_acc: 0.7747 - val_loss: 13.2977 - val_gender_output_loss: 1.3715 - val_image_quality_output_loss: 1.6788 - val_age_output_loss: 2.1357 - val_weight_output_loss: 1.5309 - val_bag_output_loss: 1.7802 - val_footwear_output_loss: 2.1128 - val_bodypose_output_loss: 1.5308 - val_emotion_output_loss: 1.1570 - val_gender_output_acc: 0.6804 - val_image_quality_output_acc: 0.4279 - val_age_output_acc: 0.2873 - val_weight_output_acc: 0.6129 - val_bag_output_acc: 0.6038 - val_footwear_output_acc: 0.5217 - val_bodypose_output_acc: 0.6699 - val_emotion_output_acc: 0.7011\n",
            "Epoch 21/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 3.6821 - gender_output_loss: 0.1692 - image_quality_output_loss: 0.5281 - age_output_loss: 0.8050 - weight_output_loss: 0.5311 - bag_output_loss: 0.4217 - footwear_output_loss: 0.4234 - bodypose_output_loss: 0.2334 - emotion_output_loss: 0.5701 - gender_output_acc: 0.9323 - image_quality_output_acc: 0.7759 - age_output_acc: 0.6893 - weight_output_acc: 0.7942 - bag_output_acc: 0.8352 - footwear_output_acc: 0.8340 - bodypose_output_acc: 0.9090 - emotion_output_acc: 0.7893 - val_loss: 12.1038 - val_gender_output_loss: 0.9198 - val_image_quality_output_loss: 1.5229 - val_age_output_loss: 2.0584 - val_weight_output_loss: 1.5074 - val_bag_output_loss: 1.8870 - val_footwear_output_loss: 1.6287 - val_bodypose_output_loss: 1.3187 - val_emotion_output_loss: 1.2609 - val_gender_output_acc: 0.7324 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.2631 - val_weight_output_acc: 0.5499 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.5287 - val_bodypose_output_acc: 0.6573 - val_emotion_output_acc: 0.7072\n",
            "Epoch 22/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.4666 - gender_output_loss: 0.1557 - image_quality_output_loss: 0.4934 - age_output_loss: 0.7590 - weight_output_loss: 0.5052 - bag_output_loss: 0.3983 - footwear_output_loss: 0.3941 - bodypose_output_loss: 0.2087 - emotion_output_loss: 0.5522 - gender_output_acc: 0.9415 - image_quality_output_acc: 0.7918 - age_output_acc: 0.7097 - weight_output_acc: 0.8037 - bag_output_acc: 0.8471 - footwear_output_acc: 0.8401 - bodypose_output_acc: 0.9218 - emotion_output_acc: 0.7967\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 3.4678 - gender_output_loss: 0.1555 - image_quality_output_loss: 0.4939 - age_output_loss: 0.7594 - weight_output_loss: 0.5055 - bag_output_loss: 0.3985 - footwear_output_loss: 0.3940 - bodypose_output_loss: 0.2096 - emotion_output_loss: 0.5514 - gender_output_acc: 0.9417 - image_quality_output_acc: 0.7915 - age_output_acc: 0.7094 - weight_output_acc: 0.8035 - bag_output_acc: 0.8470 - footwear_output_acc: 0.8404 - bodypose_output_acc: 0.9215 - emotion_output_acc: 0.7972 - val_loss: 13.4095 - val_gender_output_loss: 1.4872 - val_image_quality_output_loss: 1.8761 - val_age_output_loss: 2.2334 - val_weight_output_loss: 1.9190 - val_bag_output_loss: 1.2998 - val_footwear_output_loss: 1.8132 - val_bodypose_output_loss: 1.5534 - val_emotion_output_loss: 1.2274 - val_gender_output_acc: 0.6734 - val_image_quality_output_acc: 0.3942 - val_age_output_acc: 0.3286 - val_weight_output_acc: 0.5645 - val_bag_output_acc: 0.6079 - val_footwear_output_acc: 0.4985 - val_bodypose_output_acc: 0.6613 - val_emotion_output_acc: 0.6905\n",
            "Epoch 23/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.2968 - gender_output_loss: 0.1555 - image_quality_output_loss: 0.4658 - age_output_loss: 0.7109 - weight_output_loss: 0.4717 - bag_output_loss: 0.3728 - footwear_output_loss: 0.3804 - bodypose_output_loss: 0.2027 - emotion_output_loss: 0.5370 - gender_output_acc: 0.9380 - image_quality_output_acc: 0.8094 - age_output_acc: 0.7303 - weight_output_acc: 0.8214 - bag_output_acc: 0.8564 - footwear_output_acc: 0.8533 - bodypose_output_acc: 0.9210 - emotion_output_acc: 0.7988\n",
            "360/360 [==============================] - 35s 98ms/step - loss: 3.2982 - gender_output_loss: 0.1556 - image_quality_output_loss: 0.4660 - age_output_loss: 0.7110 - weight_output_loss: 0.4716 - bag_output_loss: 0.3733 - footwear_output_loss: 0.3809 - bodypose_output_loss: 0.2028 - emotion_output_loss: 0.5372 - gender_output_acc: 0.9378 - image_quality_output_acc: 0.8095 - age_output_acc: 0.7303 - weight_output_acc: 0.8214 - bag_output_acc: 0.8562 - footwear_output_acc: 0.8531 - bodypose_output_acc: 0.9207 - emotion_output_acc: 0.7990 - val_loss: 14.4532 - val_gender_output_loss: 1.5722 - val_image_quality_output_loss: 1.8856 - val_age_output_loss: 2.3885 - val_weight_output_loss: 1.8610 - val_bag_output_loss: 2.2648 - val_footwear_output_loss: 1.7598 - val_bodypose_output_loss: 1.5216 - val_emotion_output_loss: 1.1997 - val_gender_output_acc: 0.6628 - val_image_quality_output_acc: 0.4657 - val_age_output_acc: 0.2571 - val_weight_output_acc: 0.5701 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.5358 - val_bodypose_output_acc: 0.6240 - val_emotion_output_acc: 0.6341\n",
            "Epoch 24/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 3.0690 - gender_output_loss: 0.1392 - image_quality_output_loss: 0.4319 - age_output_loss: 0.6704 - weight_output_loss: 0.4286 - bag_output_loss: 0.3496 - footwear_output_loss: 0.3584 - bodypose_output_loss: 0.1999 - emotion_output_loss: 0.4910 - gender_output_acc: 0.9450 - image_quality_output_acc: 0.8253 - age_output_acc: 0.7474 - weight_output_acc: 0.8337 - bag_output_acc: 0.8664 - footwear_output_acc: 0.8577 - bodypose_output_acc: 0.9221 - emotion_output_acc: 0.8203 - val_loss: 14.5593 - val_gender_output_loss: 1.6300 - val_image_quality_output_loss: 1.9666 - val_age_output_loss: 2.2906 - val_weight_output_loss: 1.9333 - val_bag_output_loss: 2.0293 - val_footwear_output_loss: 1.9872 - val_bodypose_output_loss: 1.4144 - val_emotion_output_loss: 1.3079 - val_gender_output_acc: 0.6749 - val_image_quality_output_acc: 0.4299 - val_age_output_acc: 0.2697 - val_weight_output_acc: 0.4118 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.4879 - val_bodypose_output_acc: 0.6426 - val_emotion_output_acc: 0.6956\n",
            "Epoch 25/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 2.8382 - gender_output_loss: 0.1265 - image_quality_output_loss: 0.4076 - age_output_loss: 0.6226 - weight_output_loss: 0.4087 - bag_output_loss: 0.3184 - footwear_output_loss: 0.3257 - bodypose_output_loss: 0.1696 - emotion_output_loss: 0.4590 - gender_output_acc: 0.9501 - image_quality_output_acc: 0.8381 - age_output_acc: 0.7688 - weight_output_acc: 0.8448 - bag_output_acc: 0.8811 - footwear_output_acc: 0.8727 - bodypose_output_acc: 0.9379 - emotion_output_acc: 0.8308 - val_loss: 15.4228 - val_gender_output_loss: 2.2448 - val_image_quality_output_loss: 1.9741 - val_age_output_loss: 2.4748 - val_weight_output_loss: 1.8158 - val_bag_output_loss: 1.8704 - val_footwear_output_loss: 2.3818 - val_bodypose_output_loss: 1.3437 - val_emotion_output_loss: 1.3173 - val_gender_output_acc: 0.6356 - val_image_quality_output_acc: 0.4758 - val_age_output_acc: 0.2384 - val_weight_output_acc: 0.4844 - val_bag_output_acc: 0.6094 - val_footwear_output_acc: 0.5181 - val_bodypose_output_acc: 0.6583 - val_emotion_output_acc: 0.6482\n",
            "Epoch 26/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.6549 - gender_output_loss: 0.1241 - image_quality_output_loss: 0.3830 - age_output_loss: 0.5752 - weight_output_loss: 0.3765 - bag_output_loss: 0.2911 - footwear_output_loss: 0.3018 - bodypose_output_loss: 0.1729 - emotion_output_loss: 0.4302 - gender_output_acc: 0.9506 - image_quality_output_acc: 0.8470 - age_output_acc: 0.7874 - weight_output_acc: 0.8573 - bag_output_acc: 0.8912 - footwear_output_acc: 0.8850 - bodypose_output_acc: 0.9340 - emotion_output_acc: 0.8414Epoch 26/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 2.6560 - gender_output_loss: 0.1240 - image_quality_output_loss: 0.3830 - age_output_loss: 0.5755 - weight_output_loss: 0.3763 - bag_output_loss: 0.2910 - footwear_output_loss: 0.3020 - bodypose_output_loss: 0.1732 - emotion_output_loss: 0.4309 - gender_output_acc: 0.9507 - image_quality_output_acc: 0.8469 - age_output_acc: 0.7873 - weight_output_acc: 0.8574 - bag_output_acc: 0.8911 - footwear_output_acc: 0.8850 - bodypose_output_acc: 0.9338 - emotion_output_acc: 0.8411 - val_loss: 18.2554 - val_gender_output_loss: 3.1295 - val_image_quality_output_loss: 2.0063 - val_age_output_loss: 2.5566 - val_weight_output_loss: 2.0699 - val_bag_output_loss: 2.8819 - val_footwear_output_loss: 2.5965 - val_bodypose_output_loss: 1.5570 - val_emotion_output_loss: 1.4577 - val_gender_output_acc: 0.6114 - val_image_quality_output_acc: 0.4950 - val_age_output_acc: 0.2540 - val_weight_output_acc: 0.4914 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.5111 - val_bodypose_output_acc: 0.6416 - val_emotion_output_acc: 0.5670\n",
            "Epoch 27/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 2.4841 - gender_output_loss: 0.1241 - image_quality_output_loss: 0.3506 - age_output_loss: 0.5311 - weight_output_loss: 0.3467 - bag_output_loss: 0.2908 - footwear_output_loss: 0.2796 - bodypose_output_loss: 0.1573 - emotion_output_loss: 0.4040 - gender_output_acc: 0.9520 - image_quality_output_acc: 0.8641 - age_output_acc: 0.8007 - weight_output_acc: 0.8719 - bag_output_acc: 0.8891 - footwear_output_acc: 0.8947 - bodypose_output_acc: 0.9424 - emotion_output_acc: 0.8475 - val_loss: 19.0748 - val_gender_output_loss: 2.4024 - val_image_quality_output_loss: 2.2013 - val_age_output_loss: 2.8400 - val_weight_output_loss: 2.2479 - val_bag_output_loss: 2.2569 - val_footwear_output_loss: 3.9241 - val_bodypose_output_loss: 1.6182 - val_emotion_output_loss: 1.5840 - val_gender_output_acc: 0.6391 - val_image_quality_output_acc: 0.3730 - val_age_output_acc: 0.2485 - val_weight_output_acc: 0.4834 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.4798 - val_bodypose_output_acc: 0.6542 - val_emotion_output_acc: 0.6794\n",
            "Epoch 28/30\n",
            "360/360 [==============================] - 36s 100ms/step - loss: 2.3211 - gender_output_loss: 0.1047 - image_quality_output_loss: 0.3309 - age_output_loss: 0.4971 - weight_output_loss: 0.3359 - bag_output_loss: 0.2632 - footwear_output_loss: 0.2693 - bodypose_output_loss: 0.1464 - emotion_output_loss: 0.3736 - gender_output_acc: 0.9592 - image_quality_output_acc: 0.8698 - age_output_acc: 0.8183 - weight_output_acc: 0.8744 - bag_output_acc: 0.8978 - footwear_output_acc: 0.8955 - bodypose_output_acc: 0.9478 - emotion_output_acc: 0.8638 - val_loss: 15.9268 - val_gender_output_loss: 1.4147 - val_image_quality_output_loss: 2.2776 - val_age_output_loss: 2.7424 - val_weight_output_loss: 1.9568 - val_bag_output_loss: 1.6898 - val_footwear_output_loss: 2.4183 - val_bodypose_output_loss: 1.4648 - val_emotion_output_loss: 1.9624 - val_gender_output_acc: 0.7082 - val_image_quality_output_acc: 0.4088 - val_age_output_acc: 0.2807 - val_weight_output_acc: 0.5081 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.5282 - val_bodypose_output_acc: 0.6724 - val_emotion_output_acc: 0.7092\n",
            "Epoch 29/30\n",
            "360/360 [==============================] - 35s 98ms/step - loss: 2.1992 - gender_output_loss: 0.1167 - image_quality_output_loss: 0.3061 - age_output_loss: 0.4528 - weight_output_loss: 0.3091 - bag_output_loss: 0.2493 - footwear_output_loss: 0.2525 - bodypose_output_loss: 0.1524 - emotion_output_loss: 0.3603 - gender_output_acc: 0.9540 - image_quality_output_acc: 0.8799 - age_output_acc: 0.8378 - weight_output_acc: 0.8843 - bag_output_acc: 0.9050 - footwear_output_acc: 0.9010 - bodypose_output_acc: 0.9420 - emotion_output_acc: 0.8668 - val_loss: 19.1438 - val_gender_output_loss: 2.5176 - val_image_quality_output_loss: 2.3035 - val_age_output_loss: 2.6409 - val_weight_output_loss: 2.9229 - val_bag_output_loss: 2.4865 - val_footwear_output_loss: 3.1082 - val_bodypose_output_loss: 1.1800 - val_emotion_output_loss: 1.9841 - val_gender_output_acc: 0.6371 - val_image_quality_output_acc: 0.4814 - val_age_output_acc: 0.2954 - val_weight_output_acc: 0.4189 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.4980 - val_bodypose_output_acc: 0.6502 - val_emotion_output_acc: 0.6966\n",
            "Epoch 30/30\n",
            "360/360 [==============================] - 36s 99ms/step - loss: 2.0280 - gender_output_loss: 0.1030 - image_quality_output_loss: 0.2875 - age_output_loss: 0.4177 - weight_output_loss: 0.2731 - bag_output_loss: 0.2245 - footwear_output_loss: 0.2413 - bodypose_output_loss: 0.1554 - emotion_output_loss: 0.3254 - gender_output_acc: 0.9605 - image_quality_output_acc: 0.8901 - age_output_acc: 0.8494 - weight_output_acc: 0.8999 - bag_output_acc: 0.9152 - footwear_output_acc: 0.9086 - bodypose_output_acc: 0.9411 - emotion_output_acc: 0.8817 - val_loss: 17.3583 - val_gender_output_loss: 1.4824 - val_image_quality_output_loss: 2.3319 - val_age_output_loss: 3.0518 - val_weight_output_loss: 2.3095 - val_bag_output_loss: 2.3822 - val_footwear_output_loss: 2.2606 - val_bodypose_output_loss: 1.4641 - val_emotion_output_loss: 2.0757 - val_gender_output_acc: 0.7011 - val_image_quality_output_acc: 0.4612 - val_age_output_acc: 0.3357 - val_weight_output_acc: 0.5050 - val_bag_output_acc: 0.6033 - val_footwear_output_acc: 0.5519 - val_bodypose_output_acc: 0.6532 - val_emotion_output_acc: 0.6996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ac7ee0c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "outputId": "c050119c-d02a-4f76-b6c1-bd5d35a5c4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f1a54bbc4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}
